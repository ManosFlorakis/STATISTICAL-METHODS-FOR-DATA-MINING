# -*- coding: utf-8 -*-
"""Εργασία

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HcQ86aItCf9X9VduJE3g-ZuR2nPvINUE
"""

import numpy as np 
import pandas as pd
from numpy import asarray 
from sklearn.preprocessing import MinMaxScaler, StandardScaler
import matplotlib.pyplot as plt 
from sklearn.decomposition import PCA 
!pip install mlxtend
import joblib
import sys
sys.modules['sklearn.externals.joblib'] = joblib
from mlxtend.feature_selection import SequentialFeatureSelector
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
import warnings
import plotly.express as px
from sklearn.cluster import KMeans
from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso
import matplotlib
import seaborn as sns
import time
from mpl_toolkits.mplot3d import Axes3D
from sklearn.neighbors import NearestNeighbors
from sklearn.cluster import DBSCAN
from sklearn import svm
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder
from sklearn.utils import shuffle
import tensorflow as tf
from keras.utils import np_utils
from keras.models import Sequential
from keras.layers import Dense
from sklearn.metrics import matthews_corrcoef
from sklearn.metrics import confusion_matrix
from keras.utils import plot_model
from sklearn.metrics import mean_squared_error, r2_score

#!!!Σύρουμε το Αρχείο Excel στα αρχεία του Google Colab και μπορούμε να συνεχίσουμε!!!

#Load Data
require_cols = [10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,43,45]

required_df = pd.read_excel('/content/CTG.xls', sheet_name = 1, skiprows = 1,usecols = require_cols)
 
required_df=required_df.iloc[:-3,:]


print(required_df)

#1.A
# # Delete the missing record
# required_df.dropna(inplace = True)
# print(required_df)

# # Fill the missing data
# x = required_df["column_x"].mean()
# required_df["column_x"].fillna(x,inplace=True)
# print(required_df)

# Fill all the missing data
x = required_df.mean()
required_df.fillna(x,inplace=True)
print(required_df)

# 1.B
# define standard scaler
scaler = StandardScaler() 
# transform data 
standardized_df=pd.DataFrame()
standardized_df = pd.DataFrame(scaler.fit_transform(required_df.iloc[:,:-2]))
standardized_df.columns=required_df.iloc[:,:-2].columns
print(standardized_df)

# # 1.Β
# # define min max scaler (0:1)
# scaler = MinMaxScaler() 
# # transform data 
# required_df.iloc[:,:-2] = scaler.fit_transform(required_df.iloc[:,:-2]) 
# print(required_df.iloc[:,:-2])

# 1.C
# PCA ( Principal Component Analysis ) 
pca = PCA(n_components=10) 
pct = pca.fit_transform(required_df.iloc[:,:-2]) 
pct = pd.DataFrame(pct)
print(pct)
# Return a vector of the variance 
explained_variance = pca.explained_variance_ratio_
explained_variance

# Binning continuing B
Binning_df=pd.DataFrame()

for i in list(range(required_df.iloc[:,:-2].shape[1])):
 Binning_df[i]= pd.qcut(required_df.iloc[:,i], q=3, precision=1,duplicates='drop')

Binning_df.columns=required_df.iloc[:,:-2].columns 
Binning_df

#2/Feature selection
#Backward Selection
df=required_df.iloc[:,:-2]
X = df.iloc[:, 0:20].values
y = df.iloc[:,-1].values
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=1)
backward_feature_selector = SequentialFeatureSelector(RandomForestClassifier(n_jobs=-1),
                                                      k_features=(1,20),
                                                      forward=False,
                                                      floating=False,
                                                      verbose=2,
                                                      scoring="accuracy",
                                                      cv= 5).fit(X_train, y_train)

backward_feature_selector.k_feature_names_

backward_feature_selector.k_score_



#βλέπουμε ότι το αποτέλεσμα που λαμβάνουμε εμπεριέχει 4 features εκ των οποίων τα 2 είναι πανομοιότυπα και επίσης άργησε πολύ

#Μπορούμε να το δούμε και στον από κοινού πίνακα του συσχέτισης με την μέθοδο του pearson
plt.figure(figsize=(12,10))
cor = df.corr(method='pearson')
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()

#Για επιβεβαίωση θα χρησιμοποιήσουμε και άλλη μία μέθοδο
#Γραμμική Παλλινδρόμιση
X=df
reg = LassoCV()
reg.fit(X, y)
print("Best alpha using built-in LassoCV: %f" % reg.alpha_)
print("Best score using built-in LassoCV: %f" %reg.score(X,y))
coef = pd.Series(reg.coef_, index = X.columns)
print("Lasso picked " + str(sum(coef != 0)) + " variables and eliminated the other " +  str(sum(coef == 0)) + " variables")
imp_coef = coef.sort_values()

matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)
imp_coef.plot(kind = "barh")
plt.title("Feature importance using Lasso Model")

coef[coef>0]

#Όπου καθώς έχει μεγαλύτερη πλυθώρα παραγόντων και καλύτερο score θα το χρησιμοποιήσουμε και παρακάτω

df1=df[["LB","ASTV","ALTV","Mode","Median","Variance","Tendency"]]
df2=standardized_df[["LB","ASTV","ALTV","Mode","Median","Variance","Tendency"]]

#2/Outlier Detection
px.violin(data_frame=df1[["LB","ASTV","ALTV","Mode"]])

px.violin(data_frame=df1[["Median","Variance","Tendency"]])

#Από κοινού Ιστόγραμμα
kwargs = dict(histtype='stepfilled', alpha=0.4, density=True, bins=40, ec="k")

plt.hist(df1.iloc[:,0], **kwargs)
plt.hist(df1.iloc[:,1], **kwargs)
plt.hist(df1.iloc[:,2], **kwargs)
plt.hist(df1.iloc[:,3], **kwargs)
plt.hist(df1.iloc[:,4], **kwargs)
plt.hist(df1.iloc[:,5], **kwargs);

plt.show()

df1.describe()

#2 για να δούμε τον χρόνο και την απόδοση στους αλγόριθμους 
#θα χρησιμοποιήσουμε ένας από τους παρακάτω κώδικες

#3/Θα διαλέξουμε τις μεθόδους KMeans και DBScan, όπου και τα δύο είναι ιδανικά για μεγάλα δείγματα και μέτρια σε ποσότητα Clusters
#kmeans clustering method


#για να δούμε πόσα Clusters
def optimise_k_means(data,max_k):
  means=[]
  inertias=[]

  for k in range(1, max_k):
    k_means = KMeans(n_clusters=k)
    k_means.fit(data)

    means.append(k)
    inertias.append(k_means.inertia_)

  #Elbow Plot
  fig=plt.subplots(figsize=(10, 5))
  plt.plot(means,inertias, 'o-')
  plt.xlabel('Number of Clusters')
  plt.ylabel('Inertia')
  plt.grid(True)
  plt.show()

optimise_k_means(df2,10)

#In this plot it appears that there is an elbow or “bend” at k = 3 clusters.

start_test = time.time()
kmeans_test = KMeans(n_clusters=3)
kmeans_test.fit(standardized_df)
end_test = time.time()
print(end_test - start_test)

start = time.time()
kmeans = KMeans(n_clusters=3)
kmeans.fit(df2)
end = time.time()
print(end - start)

"""Και εδώ βλέπουμε στην διαδικασία του clustering ότι παίρνει παραπάνω χρόνο για να τρέξει στο df με τα 21 features"""

plt.scatter(x=df2['ALTV'],y=df2['Median'],c=kmeans.labels_)
plt.show()

plt.scatter(x=df2['LB'],y=df2['ALTV'],c=kmeans.labels_)
plt.show()

fig = plt.figure() # make 3d fig
ax = Axes3D(fig)
ax.scatter(df2['LB'], df2['ALTV'], df2['Mode'], c=kmeans.labels_, edgecolor='k')
fig.show()

fig = plt.figure() # make 3d fig
ax = Axes3D(fig)
ax.scatter(df2['ALTV'], df2['Variance'], df2['Mode'], c=kmeans.labels_, edgecolor='k')
fig.show()

#Πριν προχωρήσουμε με την επόμενη μέθοδο θα τρέξουμε την ανωτέρω διαδικασία και για ολόκληρο το κανονικοποιημένο δείγμα για να δούμε την ακρίβεια/απόδοση
optimise_k_means(standardized_df,10)

#In this plot it appears that there is an elbow or “bend” at k = 3 clusters.
plt.scatter(x=df2['ALTV'],y=df2['Median'],c=kmeans_test.labels_)
plt.show()

#όπου μπορούμε να δούμε ξεκάθαρα πόσο καλύτερο fit έχει το δείγμα που έχουμε επιλέξει

#3/DBSCAN
neighbors = NearestNeighbors(n_neighbors=9)
neighbors_fit = neighbors.fit(df2)
distances, indices = neighbors_fit.kneighbors(df2)
distances = np.sort(distances, axis=0)
distances = distances[:,1]
plt.plot(distances)

dbscan= DBSCAN(eps=0.6, min_samples=40)
labels = dbscan.fit_predict(df2.values)
np.unique(labels)

plt.scatter(x=df2['ALTV'],y=df2['Median'],c=labels)
plt.show()

fig = plt.figure() # make 3d fig
ax = Axes3D(fig)
ax.scatter(df2['ALTV'], df2['Variance'], df2['Mode'], c=labels, edgecolor='k')
fig.show()

#το ΚΜεανς μας αρμόζει καλύτερα

#4/Classification
X = df2.iloc[:, 0:6].values
y = required_df['CLASS'].values
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,train_size=0.8,random_state=0)

#Διαφορετικές κατανομές για kernel: ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’
class_names=np.unique(required_df['CLASS'].values)
classifier = svm.SVC(kernel="poly", C=5).fit(X_train, y_train)
yp_test = classifier.predict(X_test)
np.set_printoptions(precision=2)
# Plot non-normalized confusion matrix
titles_options = [
    ("Confusion matrix, without normalization", None),
    ("Normalized confusion matrix", "true"),
]
for title, normalize in titles_options:
    disp = ConfusionMatrixDisplay.from_estimator(
        classifier,
        X_test,
        y_test,
        display_labels=class_names,
        cmap=plt.cm.Blues,
        normalize=normalize,
    )
    disp.ax_.set_title(title)

plt.show()

"""Για τον πρώτο τρόπο αξιολόγησης του μοντέλου μπορούμε να παρατηρήσουμε στα confusion matrices πως στην διαγώνιο βρίσκεται το μεγαλύτερο πλήθος των παρατηρήσεων οπότε και έχουμε μία καλή προσαρμογή 
Example of confusion matrix usage to evaluate the quality of the output of a classifier on the iris data set. The diagonal elements represent the number of points for which the predicted label is equal to the true label, while off-diagonal elements are those that are mislabeled by the classifier. The higher the diagonal values of the confusion matrix the better, indicating many correct predictions
"""

#2ος τρόπος
accu=accuracy_score(y,classifier.predict(X))
print("Accuracy : %0.1f%% " % ( accu * 100))

"""2ος τρόπος αξιολόγησης
Classification Accuracy
Classification Accuracy is the simplest out of all the methods of evaluating the accuracy, and the most commonly used. Classification accuracy is simply the number of correct predictions divided by all predictions or a ratio of correct predictions to total predictions.

While it can give you a quick idea of how your classifier is performing, it is best used when the number of observations/examples in each class is roughly equivalent. Because this doesn't happen very often, you're probably better off using another metric.
"""

#Θα διενεργήσουμε επίσης βάσει υπόθεσης εκ νέου τον κώδικα και για το ολόκληρο κανονικοποιημένο δείγμα ως κάτωθι
Xf = df.iloc[:, 0:20].values
yf = required_df['CLASS'].values
Xf_train, Xf_test, yf_train, yf_test = train_test_split(Xf,yf,test_size=0.2,train_size=0.8,random_state=0)
class_namesf=np.unique(required_df['CLASS'].values)
classifierf = svm.SVC(kernel="poly", C=5).fit(Xf_train, yf_train)
yp_test = classifierf.predict(Xf_test)
np.set_printoptions(precision=2)
# Plot non-normalized confusion matrix
titles_options = [
    ("Confusion matrix, without normalization", None),
    ("Normalized confusion matrix", "true"),
]
for title, normalize in titles_options:
    disp = ConfusionMatrixDisplay.from_estimator(
        classifierf,
        Xf_test,
        yf_test,
        display_labels=class_namesf,
        cmap=plt.cm.Blues,
        normalize=normalize,
    )
    disp.ax_.set_title(title)

plt.show()

accu=accuracy_score(yf,classifierf.predict(Xf))
print("Accuracy : %0.1f%% " % ( accu * 100))

#5/Classification – Υλοποίηση με keras/tensorflow
df4=df2
df4['Class'] = required_df['CLASS']

data = df4.values
data = shuffle(data)
X = data[:,0:6].astype(float)
Y = data[:,7]

# Convert data to dummy variables
encoder = LabelEncoder().fit(Y)
y_bool = encoder.transform(Y)
y = np_utils.to_categorical(y_bool)

# Split data into training, validation and testing
len_data = data.shape[0]
print(len_data)

train_size = int(len_data * .6)
valid_size = int(len_data * .1)
print ("Train size: %d" % train_size)
print ("Validation size: %d" % valid_size)
print ("Test size: %d" % (len_data - (train_size+valid_size)))

xtr = X[:train_size,:]
ytr = y[:train_size]

xva = X[train_size:train_size+valid_size,:]
yva = y[train_size:train_size+valid_size]

xte = X[train_size+valid_size:,:]
yte = y[train_size+valid_size:]

# Define model
model = Sequential()
model.add(Dense(50, input_dim=xtr.shape[1], activation='relu'))
model.add(Dense(10, activation='relu'))
model.add(Dense(10, activation='softmax'))

# Compile model
#CategoricalCrossentropy
model.compile(loss='mean_squared_error', optimizer='adam')

print(model.summary())

plot_model(model, to_file='model.png')

history = model.fit(xtr, ytr, validation_data=(xva, yva), epochs=50, batch_size=32, verbose=2)

# Plot training and validation loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.show()

# Evaluate and Predict
scores = model.evaluate(xtr, ytr, verbose=0) # evaluate the model
print("Train MSE: ",scores)

yva_pred = model.predict(xva, batch_size=32, verbose=0) # make predictions
print("Val MSE: ", mean_squared_error(yva, yva_pred))
print("Val R2: ", r2_score(yva, yva_pred))
yte_pred = model.predict(xte, batch_size=32, verbose=0) # make predictions
print("Test MSE: ", mean_squared_error(yte, yte_pred))
print("Test R2: ", r2_score(yte, yte_pred))

accuracy_score(np.argmax(yte, axis=1),np.argmax(yte_pred, axis=1))

# Και μπορούμε να αυξήσουμε τον αριθμό των νευρώνων ώστε να παρατηρήσουμε διαφορές

# Define model
model = Sequential()
model.add(Dense(100, input_dim=xtr.shape[1], activation='relu'))
model.add(Dense(30, activation='relu'))
model.add(Dense(10, activation='softmax'))

# Compile model
#CategoricalCrossentropy
model.compile(loss='mean_squared_error', optimizer='adam')

print(model.summary())

plot_model(model, to_file='model.png')

# Fit model
history = model.fit(xtr, ytr, validation_data=(xva, yva), epochs=50, batch_size=32, verbose=2)

# Plot training and validation loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.show()

# Evaluate and Predict
scores = model.evaluate(xtr, ytr, verbose=0) # evaluate the model
print("Train MSE: ",scores)

yva_pred = model.predict(xva, batch_size=32, verbose=0) # make predictions
print("Val MSE: ", mean_squared_error(yva, yva_pred))
print("Val R2: ", r2_score(yva, yva_pred))
yte_pred = model.predict(xte, batch_size=32, verbose=0) # make predictions
print("Test MSE: ", mean_squared_error(yte, yte_pred))
print("Test R2: ", r2_score(yte, yte_pred))

accuracy_score(np.argmax(yte, axis=1),np.argmax(yte_pred, axis=1))

#Πράγματι πέτυχαμε μικρότερο μέσο τετραγωνικό σφάλμα για την δεύτερη προσπάθεια με μεγαλύτερη ακρίβεια

#Τέλος, Θα διενεργήσουμε επίσης βάσει υπόθεσης εκ νέου τον κώδικα και για το ολόκληρο κανονικοποιημένο δείγμα ως κάτωθι με τα δεδομένα της πρώτης προσπάθειας κανονικοποίησης με keras / tensorflow
df5=standardized_df
df5['Class'] = required_df['CLASS']
data = df5.values
data = shuffle(data)
X = data[:,0:20].astype(float)
Y = data[:,21]
# Convert data to dummy variables
encoder = LabelEncoder().fit(Y)
y_bool = encoder.transform(Y)
y = np_utils.to_categorical(y_bool)
# Split data into training, validation and testing
len_data = data.shape[0]
print(len_data)

train_size = int(len_data * .6)
valid_size = int(len_data * .1)
print ("Train size: %d" % train_size)
print ("Validation size: %d" % valid_size)
print ("Test size: %d" % (len_data - (train_size+valid_size)))

xtr = X[:train_size,:]
ytr = y[:train_size]

xva = X[train_size:train_size+valid_size,:]
yva = y[train_size:train_size+valid_size]

xte = X[train_size+valid_size:,:]
yte = y[train_size+valid_size:]

# Define model
model = Sequential()
model.add(Dense(50, input_dim=xtr.shape[1], activation='relu'))
model.add(Dense(10, activation='relu'))
model.add(Dense(10, activation='softmax'))

# Compile model
#CategoricalCrossentropy
model.compile(loss='mean_squared_error', optimizer='adam')

print(model.summary())

plot_model(model, to_file='model.png')

history = model.fit(xtr, ytr, validation_data=(xva, yva), epochs=50, batch_size=32, verbose=2)

# Plot training and validation loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.show()

# Evaluate and Predict
scores = model.evaluate(xtr, ytr, verbose=0) # evaluate the model
print("Train MSE: ",scores)

yva_pred = model.predict(xva, batch_size=32, verbose=0) # make predictions
print("Val MSE: ", mean_squared_error(yva, yva_pred))
print("Val R2: ", r2_score(yva, yva_pred))
yte_pred = model.predict(xte, batch_size=32, verbose=0) # make predictions
print("Test MSE: ", mean_squared_error(yte, yte_pred))
print("Test R2: ", r2_score(yte, yte_pred))

accuracy_score(np.argmax(yte, axis=1),np.argmax(yte_pred, axis=1))